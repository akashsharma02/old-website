<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="user-scalable=no, initial-scale=1, maximum-scale=1, minimum-scale=1, width=device-width, height=device-height" />

        <title>Depth Fusion For Large Scale Environments</title>

        <script src="https://code.jquery.com/jquery-3.4.1.js" integrity="sha256-WpOohJOqMqqyKL9FccASB9O0KwACQJpFTUBLTYOVvVU=" crossorigin="anonymous"></script>

        <link rel="stylesheet" href="opencv-gsoc.css">
        <link rel="icon" type="image/png" href="../data/images/cmu-seal-r.png">

		<link href="https://fonts.googleapis.com/css2?family=Bentham&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                renderMathInElement(document.body,
                [
                  {left: "$$", right: "$$", display: true},
                  {left: "\\(", right: "\\)", display: false},
                  {left: "$", right: "$", display: false},
                  {left: "\\[", right: "\\]", display: true}
                ]
                );
            });
        </script>
    </head>
    <body>
		<div class="header">
			<div class="name">
				Depth Fusion For Large Scale Environments
			</div>
			<div class="links">
                <a href="https://summerofcode.withgoogle.com/serve/4714236871180288/">Project Proposal</a> |
			</div>
		</div>
		<div class="section">
            <div class="heading">
                Abstract
            </div>
            <p>
                This project aimed to implement spatial hashing for the TSDF volume data structure in the OpenCV RGBD module,
                and leverage the same to build a scalable submap based online 3D scene reconstruction system with little to no drift.
            </p>
            <p>
                TSDF volumes are widely agreed upon in the research community to be locally consistent with minimal drift,
                therefore a natural mapping model is a PoseGraph<sup><a href="#ref2">[2]</a></sup> of overlapping submaps,
                each representing a local section of the entire scene. This mapping model allows for periodic global optimization,
                which corrects accumulating drift retrospectively in the model,
                as new sensor measurements are incorporated.
            </p>
            <p>
                The following delineates the pipeline:
            </p>
            <img src="../data/images/opencv-large-kinfu-pipeline.png" alt="">
            <p>
            This implementation uses the existing Kinect Fusion<sup><a href="#ref1">[1]</a></sup> pipeline in OpenCV.
            </p>

		</div>
        <div class="section">
            <div class="heading">
                Implementation
            </div>
            <p>The implementation in OpenCV contains the following primitives:</p>

            <div class="subheading">2.1. Hash-table based TSDF volume</div>

            <p>
            This implementation is based on the seminal work on Voxel hashing<sup><a href="#ref7">[7]</a></sup>.
                A regular TSDF volume represents a scene as a 3D volume grid of (truncated) signed distance functions.
                These truncated signed distance functions are simply the shortest distance of each point to its closest surface.
                While this is simple to implement, this constrains a user to reconstruct scenes of limited size,
                since the volume size has to be preallocated.
            </p>

            <p>
                The Hash based volume, stores the volume as an <span class="code">unordered_map</span> of smaller TSDF volume units (<span class="code">volumeUnits</span>),
                each representing canonically 8<sup>3</sup> or 16<sup>3</sup> resolution.
                These volume units are indexed by a 3 dimensional Vector,
                which is hashed appropriately to minimise the number of hash collisions[3].
            </p>

            <p>
                This TSDF volume requires the following important functionalities:
            </p>

            <ul>
                <li>
                    <b>Integration:</b> During Integration, the TSDF volume automatically allocates new volume units,
                    checking for existing volume units in the current viewing camera frustrum.
                    Integration follows by delegating the task to individual volume units parallely,
                    once the list of volume units to be integrated has been determined.
                </li>
                <br>
                <li>
                    <b>Raycasting:</b> Raycasting is required to render the TSDF volume into a virtual image, that is used for tracking.
                    Typically, for each pixel of the to-be generated image, a ray is marched in the volume with fixed steps to
                    obtain an estimated surface distance measurement.
                    In the <span class="code">HashTSDFVolume</span> we get significant performance improvements since we can skip/jump over volume units
                    (typically around 16 voxels length) that are away from the surface.
                </li>
            </ul>

            <p>
                The following PR provides a CPU implementation of Hash-table TSDF volume in OpenCV:
            </p>

            <a href="https://github.com/opencv/opencv_contrib/pull/2566" class="bookmark">
                <div class="bookmark-info">
                    <div class="bookmark-text">
                        <div class="bookmark-title">
                            [GSoC] Add new Hashtable based TSDF volume to RGBD module by akashsharma02 · Pull Request #2566 · opencv/opencv_contrib
                        </div>
                        <div class="bookmark-description">
                            This work is part of GSoC and is potentially for the first evaluation phase of the program.
                            My mentor for the GSoC time period is: @savuor My proposal is available here: https://summerofcode.withgoogle.com/dashboard/project/6190777371197440/details/
                            Please note that this is still Work in Progress,
                            as the system is not yet reliable and does not work well.
                        </div>
                    </div>
                    <div class="bookmark-href">
                        <img src="https://github.com/favicon.ico" class="icon bookmark-icon">
                        https://github.com/opencv/opencv_contrib/pull/2566
                    </div>
                </div>
                <img src="https://avatars3.githubusercontent.com/u/5009934?s=400&amp;v=4" class="bookmark-image">
            </a>

            <div class="subheading">2.2. Submap</div>

            <p>
                The submap class is an abstraction over the <span class="code">hashTSDF</span> volume to support the <span class="code">large_kinfu</span> pipeline.
                Some questions that are especially relevant with submap based 3D reconstruction are:
            </p>

            <ol>
                <li>What is the appropriate size of the submap volume? </li>
                <li>When do you terminate and initialize a new submap?</li>
                <li>How do you track and create constraints between multiple submaps in a scene for downstream optimization of poses?</li>
            </ol>

            <p>
                We address question 1. and 2. by using a camera overlap metric, which states that if the current camera frame consistently views -
                for a threshold number of frames - a new set of volume units that are only allocated recently and haven't been part of the older
                frames, then it means that the camera must have moved to a new part of the scene.[4] Once a new submap is instantiated,
                we initialize it with a submap \(SE(3)\) pose of the frame at which it was created.
            </p>

            <p>
                We maintain a list of active submaps, all of which are simultaneously tracked at each time-step.
                The simultaneous tracking provides us with a camera pose w.r.t each submap as \(T^t_{s_i c}\)
                where \(s_i\) represents the \(i^{th}\) submap coordinate frame, \(c\)
                represents the camera coordinate frame and \(t\) represents the current time-step.
                The relative constraint at each time-step between submap \(s_j\) and \(s_i\) can be obtained as:
            </p>
            <div>
                $$T^t_{s_j s_i} = {T^t_{s_i c}}^{-1} \circ T^t_{s_j c}$$
            </div>
            <p>
                A robust estimate of the constraints between submaps over multiple timesteps is
                then obtained using a simple implementation of Iteratively Reweighted Least Squares (IRLS),
                which eliminates outlier constraint estimates using the Huber norm.<sup><a href="#ref4">[4]</a></sup>
            </p>

            <div class="subheading">2.3 PoseGraph optimization</div>

            <p>
                Once we have a scene containing dynamically created submaps, we are required to optimize the submap poses to eliminate accumulating camera tracking drift and improved reconstruction
            </p>

            <p>
                We implement a simple <span class="code">PoseGraph</span> class, and implement second order optimization methods such as <i>Gauss Newton</i>, and <i>Levenberg Marquardt</i>.
            </p>

            <p>
                The idea here is to abstract the submaps as nodes of 3D \(SE(3)\) poses, and to use the sensor measurements i.e.,
                the robust Pose Constraints between submaps, as obtained from the previous section to correct the pose estimates.
                For a given submap pair \(s_j\) and \(s_i\) with an existing pose constraint \(\hat{T}_{s_j s_i}\),
                we formulate an error metric (factor) as follows:
            </p>
            <p>
            $$r = \hat{T}{s_j s_i} \ominus (T_{s_i c} \ominus T_{s_j c})$$
            </p>
            <p>
            Where \(\ominus\) denotes the \(SE(3)\) right composition i.e., \(A \ominus B \triangleq Log(B^{-1} \circ A)\)<sup><a href="#ref5">[5]</a></sup>
            </p>

            <p>
                We minimize the residual rr﻿ by linearizing the function and then solving the linear system of equations using a
                <i>Cholesky</i> solver or a QR solver.
                (We leverage <span class="code">Eigen</span> library for the linear system solver).<sup><a href="#ref2">[2]</a></sup>
            </p>

            <p>
                <b>NOTE:</b> Currently, the implementation of <i>Levenberg Marquardt</i> is unstable,
                and thus for the time being <span class="code">Ceres</span> library is used for the same<sup><a href="#ref6">[6]</a></sup>.
                However, we will refine the implementation of the optimizer to make the module
                dependency-free in the future.
            </p>

            <p>The following Pull Request implements the <span class="code">Submap</span> and <span class="code">PoseGraph</span> optimization: </p>

            <a href="https://github.com/opencv/opencv_contrib/pull/2619">
                <div class="bookmark">
                    <div class="bookmark-info">
                        <div class="bookmark-text">
                            <div class="bookmark-title">
                                [GSoC] [WIP] Add Submaps and PoseGraph optimization for Large Scale Depth Fusion by akashsharma02 · Pull Request #2619 · opencv/opencv_contrib
                            </div>
                            <div class="bookmark-description">
                                This work is part of GSoC and is for the 2nd and 3rd part of the evaluation phase of the program.
                                My mentor for the GSoC time period is: @savuor
                                My proposal is available here: https://summerofcode.withgoogle.com/dashboard/project/6190777371197440/details/
                                Checklist before review: Track multiple submaps simultaneously Posegraph data structure and Optimizer
                                (Uses Ceres for PoseGraph optimization)
                                Add support for computing constraints between submaps (In progress/Partially done) Relocalization of tracking (Dropping for GSoC.
                            </div>
                        </div>
                        <div class="bookmark-href">
                            <img src="https://github.com/favicon.ico" class="bookmark-icon">
                            https://github.com/opencv/opencv_contrib/pull/2519
                        </div>
                    </div>
                    <img src="https://avatars3.githubusercontent.com/u/5009934?s=400&amp;v=4" class="bookmark-image">
                </div>
            </a>
        </div>
        <div class="section">
            <div class="heading">
                Extensions and Future Work
            </div>
            <p>
                A large omission in this work is the Relocalization module that is imperative to prevent spurious
                creation of submaps when the camera revisits previously created submap sections.
                I plan to add this extension after GSoC.
            </p>
            <p>
            Typically <i>relocalization</i> modules are implemented using <i>Fast Bag of Words</i> or <i>Dynamic Bag of Words (FBoW, DBoW2/3)</i> <sup><a href="#ref8">[8]</a></sup> methods,
                which maintain a vocabulary of distinct keyframes as bag of words which can be quickly queried during tracking to detect cases of
                loop closure and can be used for camera relocalization if tracking fails.
            </p>
		</div>
        <div class="section">
            <div class="heading">
                Acknowledgements
            </div>
            <p>
                It has been very pleasant working with my mentor <i>Rostislav Vasilikhin</i>,
                (and with <i>Mihai Bujanca</i>), who was enthusiastic
                and forthcoming with all the issues during the 3 months of Google Summer of Code.
            </p>
            <p>
                It was especially rewarding and pedagogic implementing modules that form the basis of most Dense SLAM systems today,
                and the added benefit of making a significant open-source contribution in the process has made summer of 2020 worthwhile and interesting.
            </p>
            <p>
                I would like to thank Google for giving me this opportunity as well!
            </p>
		</div>
        <div class="section">
            <div class="heading">
                References
            </div>
            <p id="ref1"><?xml version="1.0"?><div class="csl-bib-body" style="line-height: 2; padding-left: 1em; text-indent:-1em;">
  <div class="csl-entry">[1] Newcombe, Richard A., Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J. Davison, Pushmeet Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon. “KinectFusion: Real-Time Dense Surface Mapping and Tracking.” In <i>2011 10th IEEE International Symposium on Mixed and Augmented Reality</i>, 127–36, 2011. <a href="https://doi.org/10.1109/ISMAR.2011.6092378">https://doi.org/10.1109/ISMAR.2011.6092378</a>.</div></div></p>
            <p id="ref2"><?xml version="1.0"?><div class="csl-bib-body" style="line-height: 2; padding-left: 1em; text-indent:-1em;">
                <div class="csl-entry">[2] Dellaert, Frank, and Michael Kaess. “Factor Graphs for Robot Perception.” <i>Foundations and Trends in Robotics</i> 6, no. 1–2 (2017): 1–139. <a href="https://doi.org/10.1561/2300000043">https://doi.org/10.1561/2300000043</a>.</div></div></p>
            <p id="ref3"><?xml version="1.0"><div class="csl-bib-body" style="line-height: 2; padding-left: 1em; text-indent:-1em;">
                <div class="csl-entry">[3] Daniel James. &#x201C;Boost Hash combine&#x201D; <a href="https://www.boost.org/doc/libs/1_35_0/doc/html/boost/hash_combine_id241013.html">hash_combine</a></div></div></p>
            <p id="ref4"><?xml version="1.0"?><div class="csl-bib-body" style="line-height: 2; padding-left: 1em; text-indent:-1em;"><div class="csl-entry">[4] Kähler, Olaf, Victor A. Prisacariu, and David W. Murray. “Real-Time Large-Scale Dense 3D Reconstruction with Loop Closure.” In <i>Computer Vision – ECCV 2016</i>, edited by Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, 9912:500–516. Lecture Notes in Computer Science. Cham: Springer International Publishing, 2016. <a href="https://doi.org/10.1007/978-3-319-46484-8_30">https://doi.org/10.1007/978-3-319-46484-8_30</a>.</div></div> </p>
            <p id="ref5"><?xml version="1.0"?><div class="csl-bib-body" style="line-height: 2; padding-left: 1em; text-indent:-1em;">
                <div class="csl-entry">[5] Sol&#xE0;, Joan, et al. &#x201C;A Micro Lie Theory for State Estimation in Robotics.&#x201D; <i>ArXiv:1812.01537 [Cs]</i>, Aug. 2020, <a href="http://arxiv.org/abs/1812.01537">http://arxiv.org/abs/1812.01537</a>.</div>
</div></p>
            <p id="ref6"><?xml version="1.0"?>
<div class="csl-bib-body" style="line-height: 2; padding-left: 1em; text-indent:-1em;">
    <div class="csl-entry">[6]<i>Ceres Solver &#x2014; A Large Scale Non-Linear Optimization Library</i>. <a href="http://ceres-solver.org/">http://ceres-solver.org/</a>. Accessed 29 Aug. 2020.</div>
</div></p>
            <p id="ref7"><?xml version="1.0"?>
    <div class="csl-bib-body" style="line-height: 1.35; margin-left: 2em; text-indent:-2em;">
        <div class="csl-entry">[7] Nießner, Matthias, Michael Zollhöfer, Shahram Izadi, and Marc Stamminger. “Real-Time 3D Reconstruction at Scale Using Voxel Hashing.” <i>ACM Transactions on Graphics</i> 32, no. 6 (November 1, 2013): 1–11. <a href="https://doi.org/10.1145/2508363.2508374">https://doi.org/10.1145/2508363.2508374</a>.</div></div></p>
            <p id="ref8"><?xml version="1.0"?>
<div class="csl-bib-body" style="line-height: 1.35; margin-left: 2em; text-indent:-2em;">
    <div class="csl-entry">[8] Galvez-López, Dorian, and Juan D. Tardos. “Bags of Binary Words for Fast Place Recognition in Image Sequences.” <i>IEEE Transactions on Robotics</i> 28, no. 5 (October 2012): 1188–97. <a href="https://doi.org/10.1109/TRO.2012.2197158">https://doi.org/10.1109/TRO.2012.2197158</a>.</div></div></p>
		</div>
		<div class="footer">
			<div class="name">
				Akash Sharma
			</div>
			<div class="links">
				<a href="mailto: akashsharma@cmu.edu">Email</a> |
				<a href="data/AkashSharmaResume.pdf">CV</a> |
				<a href="https://github.com/akashsharma02" target="_blank"><nobr>GitHub</nobr></a> |
				<a href="https://linkedin.com/in/akashsharma02/" target="_blank">LinkedIn</a>
			</div>
		</div>
    </body>
</html>
